{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv/lib/python3.12/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "WARNING:absl:Type handler registry overriding type \"<class 'float'>\" collision on scalar\n",
      "WARNING:absl:Type handler registry overriding type \"<class 'bytes'>\" collision on scalar\n",
      "WARNING:absl:Type handler registry overriding type \"<class 'numpy.number'>\" collision on scalar\n",
      "2025-07-22 14:35:44.071571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "torch.multiprocessing.set_start_method('spawn')\n",
    "\n",
    "import jax\n",
    "from lob.encoding import Vocab, Message_Tokenizer\n",
    "\n",
    "from lob import inference_no_errcorr as inference\n",
    "from lob.init_train import init_train_state, load_checkpoint, load_metadata, load_args_from_checkpoint\n",
    "\n",
    "from lob import inference_no_errcorr as inference\n",
    "import lob.encoding as encoding\n",
    "import preproc as preproc\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import numpy as onp\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "\n",
    "import historical_scenario\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from flax.training.train_state import TrainState\n",
    "from lob.lobster_dataloader import LOBSTER_Dataset\n",
    "import flax.linen as nn\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(config_file):\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--config\",\n",
    "        type=str,\n",
    "        default=config_file,\n",
    "        help=\"Path to your YAML config file\"\n",
    "    )\n",
    "\n",
    "    # Используем parse_known_args вместо parse_args\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args(\"1_run_exp_aggresive_scenario.yaml\")\n",
    "with open(args.config, \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_devices:  1\n"
     ]
    }
   ],
   "source": [
    "save_folder       = cfg[\"save_folder\"]\n",
    "batch_size        = cfg[\"batch_size\"]\n",
    "n_samples         = cfg[\"n_samples\"]\n",
    "n_gen_msgs        = cfg[\"n_gen_msgs\"]\n",
    "midprice_step_size= cfg[\"midprice_step_size\"]\n",
    "num_insertions    = cfg[\"num_insertions\"]\n",
    "num_coolings      = cfg[\"num_coolings\"]\n",
    "EVENT_TYPE_i      = cfg[\"EVENT_TYPE_i\"]\n",
    "DIRECTION_i       = cfg[\"DIRECTION_i\"]\n",
    "order_volume      = cfg[\"order_volume\"]\n",
    "bsz               = cfg[\"bsz\"]\n",
    "n_messages        = cfg[\"n_messages\"]\n",
    "book_dim          = cfg[\"book_dim\"]\n",
    "n_vol_series      = cfg[\"n_vol_series\"]\n",
    "sample_top_n      = cfg[\"sample_top_n\"]\n",
    "model_size        = cfg[\"model_size\"]\n",
    "data_dir          = cfg[\"data_dir\"]\n",
    "sample_all        = cfg[\"sample_all\"]\n",
    "stock             = cfg[\"stock\"]\n",
    "tick_size         = cfg[\"tick_size\"]\n",
    "rng_seed          = cfg[\"rng_seed\"]\n",
    "ckpt_path         = cfg[\"ckpt_path\"]\n",
    "num_devices = jax.local_device_count()\n",
    "print(f'num_devices: ', num_devices)\n",
    "\n",
    "batch_size = 2\n",
    "n_samples = 4\n",
    "bsz = 8\n",
    "n_gen_msgs = 50\n",
    "midprice_step_size = 50\n",
    "num_insertions = 5\n",
    "num_coolings = 1\n",
    "EVENT_TYPE_i = 4\n",
    "DIRECTION_i = 0\n",
    "order_volume = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata from checkpoints/denim-elevator-754_czg1ss71/\n",
      "Initializing model...\n",
      "configuring standard optimization setup\n",
      "[*] Trainable Parameters: 35776312\n",
      "Loading checkpoint...\n",
      "[INFO] GPU not available. Falling back to CPU.\n"
     ]
    }
   ],
   "source": [
    "# Load metadata and model\n",
    "print(\"Loading metadata from\", ckpt_path)\n",
    "args_ckpt = load_metadata(ckpt_path)\n",
    "print(\"Initializing model...\")\n",
    "train_state, model_cls = init_train_state(\n",
    "    args_ckpt,\n",
    "    n_classes=len(Vocab()),\n",
    "    seq_len=n_messages * Message_Tokenizer.MSG_LEN,\n",
    "    book_dim=book_dim,\n",
    "    book_seq_len=n_messages,\n",
    ")\n",
    "\n",
    "import jax\n",
    "from lob import init_train\n",
    "\n",
    "def safe_deduplicate_trainstate(state):\n",
    "    try:\n",
    "        devices = jax.devices(\"gpu\")\n",
    "    except RuntimeError:\n",
    "        devices = jax.devices(\"cpu\")\n",
    "        print(\"[INFO] GPU not available. Falling back to CPU.\")\n",
    "    else:\n",
    "        print(\"[INFO] Running on GPU.\")\n",
    "    \n",
    "    return jax.device_put(\n",
    "        jax.tree.map(lambda x: x[0], state),\n",
    "        device=devices[0]\n",
    "    )\n",
    "\n",
    "init_train.deduplicate_trainstate = safe_deduplicate_trainstate\n",
    "print(\"Loading checkpoint...\")\n",
    "ckpt = load_checkpoint(train_state, ckpt_path, train=False)\n",
    "state = ckpt[\"model\"]\n",
    "model = model_cls(training=False, step_rescale=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /app/data/test_set/GOOG (37 files)\n",
      "Experiment dir: /app/data_saved/exp_156_20250722_143644\n"
     ]
    }
   ],
   "source": [
    "# prepare RNG\n",
    "rng = jax.random.PRNGKey(rng_seed)\n",
    "\n",
    "# data directory\n",
    "data_path = Path(data_dir) / stock\n",
    "data_path.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Data directory: {data_path} ({len(list(data_path.iterdir()))} files)\")\n",
    "\n",
    "# Experiment upload folder\n",
    "exp_folder = historical_scenario.create_next_experiment_folder(save_folder)\n",
    "print(\"Experiment dir:\", exp_folder)\n",
    "with open(exp_folder / \"used_config.yaml\", \"w\") as f_out:\n",
    "    yaml.dump(cfg, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset\n",
    "ds = inference.get_dataset(data_path, n_messages, (num_insertions + num_coolings) * n_gen_msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================= Scenario debugging ================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_historical_scenario(\n",
    "        n_samples: int,\n",
    "        batch_size: int,\n",
    "        ds: LOBSTER_Dataset,\n",
    "        rng: jax.dtypes.prng_key,\n",
    "        seq_len: int,\n",
    "        n_msgs: int,\n",
    "        n_gen_msgs: int,\n",
    "        train_state: TrainState,\n",
    "        model: nn.Module,\n",
    "        batchnorm: bool,\n",
    "        encoder: Dict[str, Tuple[jax.Array, jax.Array]],\n",
    "        stock_symbol: str,\n",
    "        n_vol_series: int = 500,\n",
    "        save_folder: str = './data_saved/',\n",
    "        tick_size: int = 100,\n",
    "        sample_top_n: int = -1,\n",
    "        sample_all: bool = False,\n",
    "        num_insertions: int = 2,\n",
    "        num_coolings: int = 2,\n",
    "        midprice_step_size=100,\n",
    "        EVENT_TYPE_i = 4,\n",
    "        DIRECTION_i = 0,\n",
    "        order_volume = 75\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Manual, step-by-step scenario runner: for each batch, processes messages one at a time,\n",
    "    updating the orderbook and tracking midprices, mimicking track_midprices_during_messages.\n",
    "    Saves processed messages, books, and midprices for each batch.\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    rng, rng_ = jax.random.split(rng)\n",
    "    if sample_all:\n",
    "        sample_i = jnp.arange(\n",
    "            len(ds) // batch_size * batch_size,\n",
    "            dtype=jnp.int32\n",
    "        ).reshape(-1, batch_size).tolist()\n",
    "    else:\n",
    "        assert n_samples % batch_size == 0, 'n_samples must be divisible by batch_size'\n",
    "        sample_i = jax.random.choice(\n",
    "            rng_,\n",
    "            jnp.arange(len(ds), dtype=jnp.int32),\n",
    "            shape=(n_samples // batch_size, batch_size),\n",
    "            replace=False\n",
    "        ).tolist()\n",
    "    rng, rng_ = jax.random.split(rng)\n",
    "\n",
    "    save_folder = Path(save_folder)\n",
    "    save_folder.joinpath('msgs_decoded_doubled').mkdir(exist_ok=True, parents=True)\n",
    "    save_folder.joinpath('l2_book_states_halved').mkdir(exist_ok=True, parents=True)\n",
    "    save_folder.joinpath('b_seq_gen_doubled').mkdir(exist_ok=True, parents=True)\n",
    "    save_folder.joinpath('mid_price').mkdir(exist_ok=True, parents=True)\n",
    "    base_save_folder = save_folder\n",
    "\n",
    "    for batch_i in tqdm(sample_i):\n",
    "        print('BATCH', batch_i)\n",
    "        m_seq, _, b_seq_pv, msg_seq_raw, book_l2_init = ds[batch_i]\n",
    "        m_seq = jnp.array(m_seq)\n",
    "        b_seq_pv = jnp.array(b_seq_pv)\n",
    "        msg_seq_raw = jnp.array(msg_seq_raw)\n",
    "        book_l2_init = jnp.array(book_l2_init)\n",
    "\n",
    "        #=============#\n",
    "        # Step 1: Prepare positions where to insert messages (accounting for prior insertions)\n",
    "        insertion_points = [n_msgs + (i + 1) * n_gen_msgs + i for i in range(num_insertions)]\n",
    "        insertion_points = [p for p in insertion_points if p <= msg_seq_raw.shape[1]]\n",
    "        print(f\"[BATCH {batch_i}] Inserting custom messages at: {insertion_points}\")\n",
    "\n",
    "        # Step 2: Generate placeholder message using same logic as insert_custom_end (just 14-dim msg, no book logic yet)\n",
    "        def construct_custom_msg(last_msg):\n",
    "            ORDER_ID_i = 77777777\n",
    "            EVENT_TYPE = jnp.full((batch_size,), EVENT_TYPE_i)\n",
    "            SIDE = jnp.full((batch_size,), DIRECTION_i)\n",
    "            PRICE = last_msg[:, 3]  # or use fixed jnp.full((batch_size,), 123456)\n",
    "            DISPLAY_FLAG = jnp.ones((batch_size,), dtype=jnp.int32)\n",
    "            SIZE = jnp.full((batch_size,), order_volume)\n",
    "            zeros = jnp.zeros((batch_size,), dtype=jnp.int32)\n",
    "            TIME_s = last_msg[:, 8]\n",
    "            TIME_ns = last_msg[:, 9]\n",
    "\n",
    "            msg = jnp.stack([\n",
    "                jnp.full((batch_size,), ORDER_ID_i),\n",
    "                EVENT_TYPE,\n",
    "                SIDE,\n",
    "                PRICE,\n",
    "                DISPLAY_FLAG,\n",
    "                SIZE,\n",
    "                zeros, zeros,\n",
    "                TIME_s, TIME_ns,\n",
    "                zeros, zeros, zeros, zeros,\n",
    "            ], axis=1)\n",
    "\n",
    "            return msg.astype(jnp.int32)\n",
    "\n",
    "        # Step 3: Loop and insert messages\n",
    "        for i, idx in enumerate(insertion_points):\n",
    "            custom_msg = construct_custom_msg(msg_seq_raw[:, idx - 1])\n",
    "            msg_seq_raw = jnp.concatenate([\n",
    "                msg_seq_raw[:, :idx, :],\n",
    "                custom_msg[:, None, :],  # shape (B, 1, 14)\n",
    "                msg_seq_raw[:, idx:, :]\n",
    "            ], axis=1)\n",
    "\n",
    "        print(f\"[BATCH {batch_i}] msg_seq_raw shape after insertions: {msg_seq_raw.shape}\")\n",
    "        #=============#\n",
    "\n",
    "        batch_size, T, msg_dim = msg_seq_raw.shape\n",
    "        current_book = book_l2_init\n",
    "\n",
    "        books = []\n",
    "        messages = []\n",
    "        midprices = []\n",
    "\n",
    "        for t in range(T):\n",
    "            msg = msg_seq_raw[:, t:t+1, :]\n",
    "            sim_init, sim_states = inference.get_sims_vmap(current_book, msg)\n",
    "            mid_price = inference.batched_get_safe_mid_price(sim_init, sim_states, tick_size)\n",
    "            full_l2_state = jax.vmap(sim_init.get_L2_state, in_axes=(0, None))(sim_states, current_book.shape[1])\n",
    "            current_book = full_l2_state[:, : current_book.shape[1]]\n",
    "            books.append(current_book)\n",
    "            messages.append(msg)\n",
    "            midprices.append(mid_price)\n",
    "\n",
    "        books = jnp.stack(books, axis=1)             # (batch, T, book_dim)\n",
    "        messages = jnp.concatenate(messages, axis=1) # (batch, T, msg_dim)\n",
    "        midprices = jnp.stack(midprices, axis=0)     # (T, batch)\n",
    "\n",
    "        print(f\"[BATCH {batch_i}] Finished all {T} steps\")\n",
    "        print(f\"[BATCH {batch_i}] Final messages shape: {messages.shape}\")\n",
    "        print(f\"[BATCH {batch_i}] Final books shape: {books.shape}\")\n",
    "        print(f\"[BATCH {batch_i}] Final midprices shape: {midprices.shape}\")\n",
    "\n",
    "        np.save(os.path.join(base_save_folder, 'msgs_decoded_doubled', f'msgs_decoded_doubled_batch_{batch_i}_iter_0.npy'), np.array(jax.device_get(messages)))\n",
    "        np.save(os.path.join(base_save_folder, 'l2_book_states_halved', f'l2_book_states_halved_batch_{batch_i}_iter_0.npy'), np.array(jax.device_get(books)))\n",
    "        np.save(os.path.join(base_save_folder, 'mid_price', f'mid_price_batch_{batch_i}_iter_0.npy'), np.array(jax.device_get(midprices)))\n",
    "\n",
    "        # ========================\n",
    "        transform_L2_state_batch = jax.jit(jax.vmap(preproc.transform_L2_state, in_axes=(0, None, None)), static_argnums=(1, 2))\n",
    "\n",
    "        # Get midprices for each step: (T, B) → (B, T)\n",
    "        midprices_batched = midprices.T  # (B, T)\n",
    "        p_mid = midprices_batched[:, :, None]  # (B, T, 1)\n",
    "\n",
    "        # Add midprice as first column to each book state\n",
    "        books_with_mid = jnp.concatenate([p_mid, books], axis=-1)  # (B, T, 41)\n",
    "        print(f\"[BATCH {batch_i}] books_with_mid.shape: {books_with_mid.shape}\")\n",
    "\n",
    "        # Transform each book+midprice into model input format\n",
    "        books_transformed = transform_L2_state_batch(books_with_mid, n_vol_series, tick_size)  # (B, T, D)\n",
    "        print(f\"[BATCH {batch_i}] books_transformed.shape: {books_transformed.shape}\")\n",
    "\n",
    "        # Save transformed books\n",
    "        np.save(os.path.join(base_save_folder, 'b_seq_gen_doubled', f'b_seq_gen_doubled_batch_{batch_i}_iter_0.npy'), np.array(jax.device_get(books_transformed)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH [24152, 8549]\n",
      "[BATCH [24152, 8549]] Inserting custom messages at: [550, 601, 652, 703, 754]\n",
      "[BATCH [24152, 8549]] msg_seq_raw shape after insertions: (2, 805, 14)\n",
      "[BATCH [24152, 8549]] Finished all 805 steps\n",
      "[BATCH [24152, 8549]] Final messages shape: (2, 805, 14)\n",
      "[BATCH [24152, 8549]] Final books shape: (2, 805, 40)\n",
      "[BATCH [24152, 8549]] Final midprices shape: (805, 2)\n",
      "[BATCH [24152, 8549]] books_with_mid.shape: (2, 805, 41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:21<00:21, 21.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH [24152, 8549]] books_transformed.shape: (2, 805, 501)\n",
      "BATCH [9483, 19480]\n",
      "[BATCH [9483, 19480]] Inserting custom messages at: [550, 601, 652, 703, 754]\n",
      "[BATCH [9483, 19480]] msg_seq_raw shape after insertions: (2, 805, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:26<00:00, 13.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH [9483, 19480]] Finished all 805 steps\n",
      "[BATCH [9483, 19480]] Final messages shape: (2, 805, 14)\n",
      "[BATCH [9483, 19480]] Final books shape: (2, 805, 40)\n",
      "[BATCH [9483, 19480]] Final midprices shape: (805, 2)\n",
      "[BATCH [9483, 19480]] books_with_mid.shape: (2, 805, 41)\n",
      "[BATCH [9483, 19480]] books_transformed.shape: (2, 805, 501)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = run_historical_scenario(\n",
    "        n_samples,\n",
    "        batch_size,\n",
    "        ds,\n",
    "        rng,\n",
    "        n_messages * Message_Tokenizer.MSG_LEN,\n",
    "        n_messages,\n",
    "        n_gen_msgs,\n",
    "        state,\n",
    "        model,\n",
    "        args_ckpt.batchnorm,\n",
    "        Vocab().ENCODING,\n",
    "        stock,\n",
    "        n_vol_series,\n",
    "        exp_folder,\n",
    "        tick_size,\n",
    "        sample_top_n,\n",
    "        sample_all,\n",
    "        num_insertions,\n",
    "        num_coolings,\n",
    "        midprice_step_size,\n",
    "        EVENT_TYPE_i,\n",
    "        DIRECTION_i,\n",
    "        order_volume,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================= Data saving debugging ================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_inp_file = \"/app/data_saved/exp_73_20250701_210502/b_seq_gen_doubled/b_seq_inp_[14606, 16120].npy\"\n",
    "current_inp_file = \"/app/data_saved/exp_153_20250722_140141/l2_book_states_halved/l2_book_states_halved_batch_[13104, 5937]_iter_0.npy\"\n",
    "\n",
    "example_inp_file = np.load(example_inp_file)\n",
    "current_inp_file = np.load(current_inp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
